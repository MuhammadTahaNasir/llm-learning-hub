# FREE RAG Pipeline - HuggingFace + FAISS by Muhammad Taha Nasir
# This notebook creates a Retrieval-Augmented Generation (RAG) pipeline.
# It supports text and PDF inputs, embeds documents with SentenceTransformers, stores them in FAISS, and answers questions using a free local LLM (google/flan-t5-base).

# Step 1: Install dependencies
!pip install -q sentence-transformers faiss-cpu transformers langchain unstructured pdfminer.six langchain-community

# Step 2: Import libraries
import os
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import faiss
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader, UnstructuredPDFLoader
from unstructured.cleaners.core import clean_extra_whitespace

# Step 3: Function to load and process documents (text or PDF)
def load_document(file_path):
    """
    Load a text or PDF file and return its content.
    Supports .txt and .pdf formats.
    """
    try:
        if file_path.endswith(".txt"):
            loader = TextLoader(file_path)
        elif file_path.endswith(".pdf"):
            loader = UnstructuredPDFLoader(file_path)
        else:
            raise ValueError("Unsupported file format. Use .txt or .pdf")

        documents = loader.load()
        # Clean extra whitespace from the document
        for doc in documents:
            doc.page_content = clean_extra_whitespace(doc.page_content)
        return documents
    except Exception as e:
        print(f"Error loading document: {e}")
        return None

# Step 4: Function to chunk text
def chunk_text(documents, chunk_size=200, chunk_overlap=20):
    """
    Split documents into smaller chunks for embedding.
    """
    try:
        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = text_splitter.split_documents(documents)
        return [chunk.page_content for chunk in chunks]
    except Exception as e:
        print(f"Error chunking text: {e}")
        return None

# Step 5: Function to create FAISS index
def create_faiss_index(embeddings):
    """
    Create and populate a FAISS index with embeddings.
    """
    try:
        dim = embeddings[0].shape[0]
        index = faiss.IndexFlatL2(dim)
        index.add(np.array(embeddings))
        return index
    except Exception as e:
        print(f"Error creating FAISS index: {e}")
        return None

# Step 6: Function to query the RAG pipeline
def query_rag_pipeline(query, texts, index, model, qa_pipeline, k=3):
    """
    Query the RAG pipeline with a question and return the answer.
    """
    try:
        # Embed the query
        query_embedding = model.encode([query])
        # Search for top-k relevant chunks
        D, I = index.search(np.array(query_embedding), k=k)
        # Retrieve relevant text
        context = "\n".join([texts[i] for i in I[0]])
        # Create prompt for the LLM
        prompt = f"Answer the question based on the context:\n{context}\n\nQ: {query}\nA:"
        # Generate answer
        result = qa_pipeline(prompt, max_length=100)[0]['generated_text']
        return result
    except Exception as e:
        print(f"Error processing query: {e}")
        return None

# Step 7: Main pipeline execution
def main():
    # Create a sample text file (replace with your own text or PDF later)
    sample_text = """
    LangChain is a framework for developing applications powered by language models.
    It enables chaining LLMs with tools like vector databases and APIs.
    FAISS is a vector database optimized for similarity search on local machines.
    """
    with open("example.txt", "w") as f:
        f.write(sample_text)

    # Load document (replace 'example.txt' with your file path, e.g., 'sample.pdf')
    file_path = "example.txt"
    documents = load_document(file_path)
    if not documents:
        print("Failed to load document. Exiting.")
        return

    # Chunk the text
    texts = chunk_text(documents)
    if not texts:
        print("Failed to chunk text. Exiting.")
        return

    # Load embedding model
    print("Loading SentenceTransformer model...")
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(texts, show_progress_bar=True)

    # Create FAISS index
    index = create_faiss_index(embeddings)
    if not index:
        print("Failed to create FAISS index. Exiting.")
        return

    # Load question-answering model
    print("Loading google/flan-t5-base model...")
    qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base")

    # Example query
    query = "What is LangChain?"
    result = query_rag_pipeline(query, texts, index, model, qa_pipeline)
    if result:
        print(f"\nQ: {query}")
        print(f"A: {result}")

# Run the pipeline
if __name__ == "__main__":
    main()

# To use your own file:
# 1. Upload a .txt or .pdf file to Colab (e.g., via the sidebar).
# 2. Update the `file_path` variable in the `main()` function to your file's name.
# 3. Run the notebook again!
