# ðŸ¤— Transformers 101 â€” HuggingFace Quickstart
# ðŸ“˜ Notebook: Learn to use HuggingFace Transformers pipelines for common NLP tasks
# ðŸ”§ Designed to run in Google Colab with free models

# âœ… Step 1: Install the library
!pip install -q transformers

# âœ… Step 2: Import libraries
import torch
from transformers import pipeline

# âœ… Step 3: Check for GPU availability
device = 0 if torch.cuda.is_available() else -1  # Use GPU (0) if available, else CPU (-1)
print(f"Using device: {'GPU' if device == 0 else 'CPU'}")

# ðŸ”¹ Helper function to run pipelines and handle errors
def run_pipeline(pipeline_obj, input_data, **kwargs):
    try:
        result = pipeline_obj(input_data, **kwargs)[0]
        print(f"\nInput:\n{input_data}\n\nResult:\n{result}\n")
        return result
    except Exception as e:
        print(f"\nInput:\n{input_data}\n\nError:\n{str(e)}\n")
        return None

# ---
# ðŸ”¹ Example 1: Text Generation (Translation)
# ðŸ“ Translate a sentence using flan-t5-base
text_gen = pipeline("text2text-generation", model="google/flan-t5-base", device=device)
prompt = "Translate to German: I love natural language processing."
run_pipeline(text_gen, prompt, max_new_tokens=50)

# ðŸ”¹ Example 2: Summarization
# ðŸ“ Summarize a paragraph using distilbart
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6", device=device)
long_text = """
Transformers are a type of deep learning model that have revolutionized natural language processing by enabling models to learn contextual relationships between words in a text.
"""
run_pipeline(summarizer, long_text, max_length=20, min_length=10)

# ðŸ”¹ Example 3: Question Answering
# ðŸ“ Answer a question based on context
qa = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad", device=device)
context = "HuggingFace Transformers provide thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, and more."
question = "What tasks can HuggingFace Transformers perform?"
run_pipeline(qa, {"question": question, "context": context})

# ðŸ”¹ Example 4: Sentiment Analysis
# ðŸ“ Analyze sentiment of a sentence
sentiment = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english", device=device)
text = "This notebook is amazing!"
run_pipeline(sentiment, text)

# ðŸ”¹ Example 5: Named Entity Recognition (NER)
# ðŸ“ Identify entities in a sentence
ner = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english", aggregation_strategy="simple", device=device)
text = "HuggingFace is based in New York and was founded by ClÃ©ment Delangue."
run_pipeline(ner, text)

# ---
# ðŸ“š Tips for Using This Notebook
# 1. Run in Colab with GPU enabled (Runtime > Change runtime type > GPU) for faster inference.
# 2. Experiment with different inputs or models (e.g., google/flan-t5-large for text generation).
# 3. Check model cards on HuggingFace (e.g., https://huggingface.co/google/flan-t5-base) for task suitability.
