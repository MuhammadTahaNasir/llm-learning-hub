{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e4173a0"
      },
      "source": [
        "# ğŸ§  Fine-Tune a Transformer (HuggingFace Trainer)\n",
        "# ğŸ“˜ Yo, friend! Ready to teach a model to vibe with movie reviews? ğŸ¬\n",
        "# ğŸŒŸ We're fine-tuning DistilBERT on the IMDB dataset for sentiment analysis.\n",
        "\n",
        "# Step 1: Install the tools we need\n",
        "# Grabbing HuggingFace's transformers, datasets, and evaluate for metrics. Let's roll!\n",
        "!pip install -q transformers datasets evaluate fsspec\n",
        "\n",
        "# Step 2: Import our goodies\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "print(\"ğŸ‰ Libraries loaded â€” time to make some magic!\")\n",
        "\n",
        "# Step 3: Check for GPU (faster training!)\n",
        "device = torch.device(\"cuda\" if torch.cuda.cuda_is_available() else \"cpu\")\n",
        "print(f\"ğŸš€ Running on {device} â€” letâ€™s make it happen!\")\n",
        "\n",
        "# Step 4: Download and load the IMDB dataset manually\n",
        "# Downloading the dataset files directly as loading via load_dataset(\"imdb\") is causing issues.\n",
        "try:\n",
        "    # Check if the dataset directory already exists to avoid re-downloading\n",
        "    if not os.path.exists(\"aclImdb\"):\n",
        "        !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "        !tar -xf aclImdb_v1.tar.gz\n",
        "        print(\"\\nContents of aclImdb directory after extraction:\")\n",
        "        !ls aclImdb\n",
        "    else:\n",
        "        print(\"âœ… Dataset directory 'aclImdb' already exists.\")\n",
        "\n",
        "    def load_imdb_data(directory):\n",
        "        reviews = []\n",
        "        labels = []\n",
        "        for label in ['pos', 'neg']:\n",
        "            subdir = os.path.join(directory, label)\n",
        "            for filename in os.listdir(subdir):\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    with open(os.path.join(subdir, filename), 'r', encoding='utf-8') as f:\n",
        "                        reviews.append(f.read())\n",
        "                    labels.append(1 if label == 'pos' else 0)\n",
        "        return pd.DataFrame({'text': reviews, 'label': labels})\n",
        "\n",
        "    train_df = load_imdb_data(\"aclImdb/train\")\n",
        "    test_df = load_imdb_data(\"aclImdb/test\")\n",
        "\n",
        "    # Create HuggingFace Datasets directly from pandas DataFrames\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Combine into a DatasetDict\n",
        "    dataset = DatasetDict({\n",
        "        \"train\": train_dataset,\n",
        "        \"test\": test_dataset\n",
        "    })\n",
        "\n",
        "    print(f\"ğŸ“š Loaded IMDB dataset with {len(dataset['train'])} train and {len(dataset['test'])} test samples!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Failed to load dataset: {e}\")\n",
        "    print(\"ğŸ‘‰ Try restarting Colab (Runtime > Restart session) or checking your internet. If it persists, weâ€™ll find another way!\")\n",
        "    raise\n",
        "\n",
        "# Step 5: Grab a smaller chunk to keep Colab happy\n",
        "# Using 2000 train and 1000 test samples to avoid memory hiccups.\n",
        "try:\n",
        "    small_train = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "    small_test = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "    print(f\"âœ‚ï¸ Using {len(small_train)} train and {len(small_test)} test samples â€” nice and lightweight!\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Failed to split dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 6: Load tokenizer and model\n",
        "# DistilBERT is a speedy, small version of BERT â€” perfect for Colabâ€™s free tier.\n",
        "try:\n",
        "    model_ckpt = \"distilbert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "    # Move the model to the determined device (GPU or CPU)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2).to(device)\n",
        "    print(\"ğŸ§  Loaded DistilBERT model and tokenizer â€” ready to learn those reviews!\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Failed to load model/tokenizer: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 7: Tokenize the reviews\n",
        "# Turning text into numbers the model can understand (like teaching it movie lingo).\n",
        "def tokenize_fn(batch):\n",
        "    # Ensure tokenizer output is moved to the correct device\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "try:\n",
        "    # Apply tokenization and move to device\n",
        "    tokenized_train = small_train.map(tokenize_fn, batched=True)\n",
        "    tokenized_test = small_test.map(tokenize_fn, batched=True)\n",
        "    print(\"âœ¨ Tokenized datasets â€” we're speaking DistilBERTâ€™s language now!\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Tokenization failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 8: Set up accuracy metric\n",
        "# Letâ€™s measure how well our model predicts positive vs. negative vibes.\n",
        "try:\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        # Ensure predictions and labels are on the same device for metric computation\n",
        "        preds = torch.argmax(torch.tensor(logits, device=device), axis=1)\n",
        "        return accuracy.compute(predictions=preds.cpu().numpy(), references=labels) # Move to CPU for numpy conversion\n",
        "\n",
        "    print(\"ğŸ“Š Accuracy metric ready â€” ready to check our score!\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Failed to load metric: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 9: Set up the training pipeline\n",
        "# This is our study plan for DistilBERT to learn the movie review game.\n",
        "try:\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=8,  # Low for CPU stability\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=2,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        # Add device argument to TrainingArguments for explicit device placement\n",
        "        # This is often handled automatically by Trainer, but being explicit can help\n",
        "        # device=device # This line might not be necessary and can cause issues with Trainer\n",
        "        report_to=\"none\",  # Disable reporting to avoid W&B login\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    print(\"âš™ï¸ Training pipeline ready â€” time to train like a champ!\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Failed to set up trainer: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 10: Train the model\n",
        "# Letâ€™s teach DistilBERT to predict those movie review sentiments!\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"ğŸ‰ Training complete â€” our modelâ€™s got some serious movie review skills now!\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Training failed: {e}\")\n",
        "    print(\"ğŸ‘‰ Try reducing batch size to 4 or restarting Colab if memory runs out!\")\n",
        "    raise\n",
        "\n",
        "# Step 11: Evaluate the model\n",
        "# How good is our model at predicting sentiments? Letâ€™s find out!\n",
        "try:\n",
        "    results = trainer.evaluate()\n",
        "    print(f\"\\nğŸ“ˆ Evaluation results: {results}\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Evaluation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 12: Test with your own review\n",
        "# Throw in a movie review and see what the model thinks!\n",
        "try:\n",
        "    test_review = \"I absolutely loved this movie, it was fantastic!\"\n",
        "    # Ensure input tensors are on the correct device\n",
        "    inputs = tokenizer(test_review, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "    outputs = model(**inputs)\n",
        "    prediction = outputs.logits.argmax().item()\n",
        "    print(f\"\\nğŸ¥ Your review: '{test_review}'\")\n",
        "    print(f\"ğŸ¤– Prediction: {'Positive' if prediction == 1 else 'Negative'}\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ˜• Prediction failed: {e}\")\n",
        "\n",
        "# ğŸ“š Tips for Having Fun\n",
        "# - Got a GPU? Try a bigger dataset (e.g., 5000 train samples) in Step 5.\n",
        "# - Play with Step 12: Test reviews like \"This movie was awful!\" or your own.\n",
        "# - Want a bigger model? Swap to bert-base-uncased (needs more memory).\n",
        "# - Dive into HuggingFaceâ€™s Trainer docs (https://huggingface.co/docs/transformers/main_classes/trainer) for pro tips!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}